# Data Engineering Project - Team Galena

In this group project, we created a data platform to **Extract** data from an operational database into a data lake, **Transform** the data into a star schema, and **Load** the remodelled it into a data warehouse hosted in AWS, allowing highly efficient **OLAP**. 

## Pipeline Overview 
ğŸŒŠ **Ingestion - Data Lake** 
- AWS Ingestion Lambda 
- Triggered every 15 minutes using CloudWatch logs and EventBridge schedule
- Retrieves db credentials from AWS Secrets Manager using boto3
- Creates connection with PostgreSQL db using pg8000
- Extracts tables and writes to S3 ingestion bucket with timestamped file names

ğŸª„ **Transformation** 
- AWS Transformation Lambda invoked by each ingestion 
- Loads most recently ingested tables using timestamps lookup file 
- Wrangles and cleans data, then transforms into star schema
- Writes transformed tables to S3 transformation bucket with timestamped file names

â­ï¸ **Data Warehouse - Star Schema**
- AWS Warehouse Lambda triggered with each transformation
- Loads most recently transformed dimension and fact tables
- Writes new data to RDS PostgreSQL Data Warehouse

ğŸ“Š **Data Analysis** 
- Visualisations in BI Tableau
- Summary statistics and trends in the data

## Dataset 
ğŸ”¸ **DB name:** totesys<br>
ğŸ”¸ **Description:** PostgreSQL db hosted in RDS, updated several times a day 

## Project Structure
```bash
team-galena-group-project/
â”‚
â”œâ”€â”€ .github/			        # CI/CD GitHub Actions
â””â”€â”€ â””â”€â”€ ci.yml
â”‚
â”œâ”€â”€ infrastructure/			    # IaC using Terraform
â”‚   â”œâ”€â”€ build/
â”‚   â”œâ”€â”€ config.tf
â”‚   â”œâ”€â”€ eventbridge.tf
â”‚   â”œâ”€â”€ iam.tf
â”‚   â”œâ”€â”€ lambda.tf
â”‚   â””â”€â”€ s3.tf
â”‚  
â”œâ”€â”€ src/			            # data processing pipeline
â”‚   â””â”€â”€ ingestion/
â”‚      â””â”€â”€ ingest.py            # ingestion script holding lambda handler
â”‚   â””â”€â”€ transformation/
â”‚       â”œâ”€â”€ utils/              # table parser and save parquet to s3 utils
â”‚       â”œâ”€â”€ dim_counterparty.py
â”‚       â”œâ”€â”€ dim_currency.py
â”‚       â”œâ”€â”€ dim_date.py
â”‚       â”œâ”€â”€ dim_design.py
â”‚       â”œâ”€â”€ dim_location.py
â”‚       â””â”€â”€ dim_staff.py
â”‚       â”œâ”€â”€ lambda_handler.py  # transformation lambda handler
â”‚       â””â”€â”€ transformer.py     # transformer script to create dimensions and facts
â”‚   â””â”€â”€ warehouse/
â”‚
â”œâ”€â”€ tests/			            # unit tests following TDD
â”‚   â””â”€â”€ unit/
â”‚
â”œâ”€â”€ venv/	
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt   
```

## How to Run the Project

**1. Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate   # on Linux/macOS
venv\Scripts\activate      # on Windows

export PYTHONPATH=$(pwd)   # Set project root
```

**2. Install dependencies**
```bash
pip install -r requirements.txt
```
**3. Create an AWS IAM user and configure credentials in terminal**

- Store Access Key ID and Secret Access Key in .aws/credentials:
```bash
[default]
aws_access_key_id = AKIAXXXXXXXXXXXXXXXX
aws_secret_access_key = <your secret access key>
```

**4. Create Secret for db credentials in AWS Secrets Manager**
- Secret name: totesys/rds/credentials

**5. Create necessary S3 buckets in AWS Console**
- galena-remote-state-backend
- galena-s3-ingestion-lambda-bucket
- galena-s3-transformation-lambda-bucket
- s3-ingestion-bucket-team-galena
- s3-transformation-bucket-team-galena

**6. Initialise, plan, and apply Terraform**
```bash
terraform init
terraform plan
terraform apply
```

## Contributors
ğŸ’» @hannahfranks<br>
ğŸ’» @J4M1N<br>
ğŸ’» @shohag1610<br>
ğŸ’» @leonie-vs<br>



